{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Q lookup table and visualize policy\n",
    "## Overview\n",
    "This notebook creates an environment with a smoke plume movie and an agent that learns to seek odor sources using RL. It's useful to see how the relevant classes are created. Q values for state-action pairs are optimized in loop in the notebook, so it also demonstrates this simple learning rule.\n",
    "## Arena\n",
    "The agent navigates an arena that is 'odorized' according to a smoke plume movie ('project_root/src/data/plume_movies/intermiitent_smoke.avi').\n",
    "## State space\n",
    "The agent detects:\n",
    "    - odor concentration (binarized into low and high)\n",
    "    - odor gradient in the cross-wind direction (discretized into crosswind A, crosswind B, neither)\n",
    "    - odor motion in the cross-wind direction (crosswind A, crosswind B, neither).\n",
    "    - the direction to a previously encountered location (coarsely represented at the quadrant level)\n",
    "## Action space\n",
    "The agent walks with constant speed and chooses a walking direction at every timestep. The walking directions are the upwind/downwind/crosswind A/crosswind B directions, as well as the intermediate directions (i.e., at 45 degree angles).\n",
    "In addition, the agent can choose to preserve a memory of a previously encountered location OR replace that memory with a memory its current location\n",
    "## Reward\n",
    "The agent receives a reward quantity at every timepoint. This is 1 on timesteps where the agent gets to the goal zone; otherwise it is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 3 9]\n"
     ]
    }
   ],
   "source": [
    "from src.models.action_definitions import WalkStopActionEnum\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "#import visdom\n",
    "import tqdm.notebook\n",
    "\n",
    "from src.models.goals import GOAL_X,GOAL_Y,GOAL_RADIUS\n",
    "from src.models.motion_environment_factory import PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardStopActionFactory\n",
    "\n",
    "N_EPISODES = 3000 # How many independently initialized runs to train on\n",
    "ALPHA = 0.1 # Learning rate\n",
    "GAMMA = 0.5 # Reward temporal discount factor\n",
    "MAX_EPSILON = 1 # Starting exploration rate\n",
    "MIN_EPSILON = 0.1 # Asymptote of decaying exploration rate\n",
    "DECAY = 0.05 # Rate of exploration decay\n",
    "\n",
    "MIN_RESET_X = GOAL_X + 10 + GOAL_RADIUS # Initialization condition\n",
    "MAX_RESET_X = 1400 # Initialization condition\n",
    "rewards = np.zeros(N_EPISODES)\n",
    "total_rewards = 0\n",
    "\n",
    "\n",
    "plume_movie_path = os.path.join('..', 'src', 'data', 'plume_movies', 'intermittent_smoke.avi')\n",
    "environment = PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardStopActionFactory(movie_file_path=plume_movie_path,actions=WalkStopActionEnum).plume_environment\n",
    "q_shape = np.append(environment.observation_space.nvec,environment.action_space.n)\n",
    "q_table = np.zeros(shape=q_shape)\n",
    "print(q_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q_SAVE_NAME = 'q_table_stopping_23_02_22.npy'\n",
    "\n",
    "save_path = os.path.join('..','trained_models',Q_SAVE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting less than 210 away in x coordinate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6004acf7af3f497b803f1760ece5fb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "hit a wall\n",
      "hit a wall\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "hit a wall\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "received reward\n",
      "hit a wall\n",
      "received reward\n",
      "received reward\n",
      "hit a wall\n",
      "hit a wall\n",
      "received reward\n",
      "hit a wall\n"
     ]
    }
   ],
   "source": [
    "epsilon = MAX_EPSILON\n",
    "alpha = ALPHA\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "min_reset_x = MIN_RESET_X\n",
    "max_reset_x = min_reset_x + GOAL_RADIUS\n",
    "print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "reset_y_radius = 400\n",
    "transition_incrementer = 0\n",
    "parameter_decay = 0\n",
    "log_interval = 25\n",
    "recent_rewards = deque(maxlen=25)\n",
    "#vis = visdom.Visdom()\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "for episode in tqdm.notebook.tqdm(range(N_EPISODES)):\n",
    "    if (transition_incrementer > 0) & (transition_incrementer % 500 == 0):\n",
    "        transition_incrementer -= 500\n",
    "        old_reset_x = max_reset_x\n",
    "        old_reset_y = reset_y_radius\n",
    "        max_reset_x = np.min([MAX_RESET_X,10*GOAL_RADIUS+max_reset_x])\n",
    "\n",
    "        if (old_reset_x != min_reset_x):\n",
    "\n",
    "            print('Making task harder')\n",
    "            print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "            parameter_decay -= 150 # Increase exploration rate following the change in initialization conditions\n",
    "\n",
    "    flip = np.random.choice([True,False],1)\n",
    "    observation = environment.reset(options={'randomization_x_bounds':np.array([min_reset_x,max_reset_x]),\n",
    "                                             'randomization_y_bounds': np.array([-reset_y_radius, reset_y_radius]) + GOAL_Y,\n",
    "                                             'flip':flip})\n",
    "\n",
    "    done = False\n",
    "    while not done: # Advance the environment (e.g., the smoke plume updates and the agent walks a step)\n",
    "        explore = rng.random() < epsilon# Can pick all random numbers at start\n",
    "        if explore:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            best = np.argwhere(q_table[tuple(observation)] == np.amax(q_table[tuple(observation)]))\n",
    "            action = rng.choice(best)\n",
    "\n",
    "            #action = np.argmax(q_table[tuple(observation)])\n",
    "            #action = np.unravel_index(action,shape=environment.action_space.n).squeeze()\n",
    "\n",
    "        new_observation, reward, done, odor_measures = environment.step(action)\n",
    "        \n",
    "        recent_rewards.append(reward)\n",
    "\n",
    "        if reward > 0:\n",
    "            transition_incrementer += 1\n",
    "            parameter_decay +=1\n",
    "            total_rewards += 1\n",
    "            print('received reward')\n",
    "        if reward < 0:\n",
    "            print('hit a wall')\n",
    "        update_index = tuple(np.append(observation,action))\n",
    "\n",
    "        t1_value_index = tuple(new_observation)# Note the use of this index requires actions to be last axes of q table\n",
    "        q_table[update_index] = \\\n",
    "            q_table[update_index] +\\\n",
    "            ALPHA * (reward + GAMMA*np.max(q_table[t1_value_index]) -\\\n",
    "            q_table[update_index])\n",
    "        observation = new_observation\n",
    "\n",
    "\n",
    "        epsilon = MIN_EPSILON + (MAX_EPSILON-MIN_EPSILON)*np.exp(-DECAY*parameter_decay)\n",
    "        \n",
    "        if episode % log_interval == 0: pass\n",
    "            #vis.line(X=np.array(episode),Y=np.array(np.mean(recent_rewards)),win='Reward',opts={'title':'Reward'})\n",
    "        \n",
    "np.save(save_path,q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(save_path,q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Below, save frames of a movie depicting the trained agent walking through a smoke plume according its learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import src.visualization.visualize_behavior\n",
    "\n",
    "print(save_path)\n",
    "POLICY_FRAMES_PARENT_DIRECTORY = 'q_policy_stopping_example_frames_23_02_22'\n",
    "\n",
    "policy_movie_path = os.path.join('..','result_images',POLICY_FRAMES_PARENT_DIRECTORY)\n",
    "if not os.path.isdir(policy_movie_path):\n",
    "    os.mkdir(policy_movie_path)\n",
    "\n",
    "src.visualization.visualize_behavior.main(movie_path=plume_movie_path,q_table_path=save_path,savepath=policy_movie_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
