{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimize Q lookup table and visualize policy\n",
    "## Overview\n",
    "This notebook creates an environment with a smoke plume movie and an agent that learns to seek odor sources using RL. It's useful to see how the relevant classes are created. Q values for state-action pairs are optimized in loop in the notebook, so it also demonstrates this simple learning rule.\n",
    "## Arena\n",
    "The agent navigates an arena that is 'odorized' according to a smoke plume movie ('project_root/src/data/plume_movies/intermiitent_smoke.avi').\n",
    "## State space\n",
    "The agent detects:\n",
    "    - odor concentration (binarized into low and high)\n",
    "    - odor gradient in the cross-wind direction (discretized into crosswind A, crosswind B, neither)\n",
    "    - and odor motion in the cross-wind direction (crosswind A, crosswind B, neither).\n",
    "## Action space\n",
    "The agent walks with constant speed and chooses a walking direction at every timestep. The walking directions are the upwind/downwind/crosswind A/crosswind B directions, as well as the intermediate directions (i.e., at 45 degree angles).\n",
    "## Reward\n",
    "The agent receives a reward quantity at every timepoint. This is 1 on timesteps where the agent gets to the goal zone; otherwise it is 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 3]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tqdm.notebook\n",
    "\n",
    "from src.models.goals import GOAL_X,GOAL_Y,GOAL_RADIUS\n",
    "from src.models.motion_environment_factory import PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardFactory\n",
    "\n",
    "N_EPISODES = 2000 # How many independently initialized runs to train on\n",
    "ALPHA = 0.1 # Learning rate\n",
    "GAMMA = 0.5 # Reward temporal discount factor\n",
    "MAX_EPSILON = 1 # Starting exploration rate\n",
    "MIN_EPSILON = 0.1 # Asymptote of decaying exploration rate\n",
    "DECAY = 0.1 # Rate of exploration decay\n",
    "\n",
    "MIN_RESET_X = GOAL_X + 10 + GOAL_RADIUS # Initialization condition\n",
    "MAX_RESET_X = 1430 # Initialization condition\n",
    "rewards = np.zeros(N_EPISODES)\n",
    "total_rewards = 0\n",
    "\n",
    "\n",
    "plume_movie_path = os.path.join('..', 'src', 'data', 'plume_movies', 'intermittent_smoke.avi')\n",
    "environment = PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardFactory(movie_file_path=plume_movie_path).plume_environment\n",
    "q_shape = np.append(environment.observation_space.nvec,environment.action_space.n)\n",
    "q_table = np.zeros(shape=q_shape)\n",
    "print(environment.observation_space.nvec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting less than 230 away in x coordinate\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d15e6342adb64f2fbcee08ba065602b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received reward\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 42\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(q_table[\u001B[38;5;28mtuple\u001B[39m(observation)])\n\u001B[0;32m---> 42\u001B[0m new_observation, reward, done, odor_measures \u001B[38;5;241m=\u001B[39m \u001B[43menvironment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reward \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     44\u001B[0m     transition_incrementer \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Dropbox (emonetlab)/users/sam_brudner/analysis/plume_rl/src/models/gym_motion_environment_classes.py:113\u001B[0m, in \u001B[0;36mPlumeMotionNavigationEnvironment.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfly_spatial_parameters\u001B[38;5;241m.\u001B[39mupdate_position(walk_displacement)\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprior_frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39modor_plume\u001B[38;5;241m.\u001B[39mframe\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43modor_plume\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39modor_features\u001B[38;5;241m.\u001B[39mupdate(sensor_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfly_spatial_parameters\u001B[38;5;241m.\u001B[39mposition,\n\u001B[1;32m    115\u001B[0m                           odor_plume_frame\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39modor_plume\u001B[38;5;241m.\u001B[39mframe,\n\u001B[1;32m    116\u001B[0m                           prior_odor_plume_frame\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprior_frame)\n\u001B[1;32m    117\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward\n",
      "File \u001B[0;32m~/Dropbox (emonetlab)/users/sam_brudner/analysis/plume_rl/src/models/odor_plumes.py:183\u001B[0m, in \u001B[0;36mOdorPlumeFromMovie.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframe_number \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 183\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvideo_capture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCAP_PROP_POS_FRAMES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mframe_number\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_frame()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "Q_SAVE_NAME = 'q_table_YY_MM_DD.npy'\n",
    "\n",
    "save_path = os.path.join('..','trained_models',Q_SAVE_NAME)\n",
    "\n",
    "epsilon = MAX_EPSILON\n",
    "alpha = ALPHA\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "min_reset_x = MIN_RESET_X\n",
    "max_reset_x = min_reset_x + GOAL_RADIUS\n",
    "print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "reset_y_radius = 400\n",
    "transition_incrementer = 0\n",
    "parameter_decay = 0\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "for episode in tqdm.notebook.tqdm(range(N_EPISODES)):\n",
    "    if (transition_incrementer > 0) & (transition_incrementer % 10 == 0):\n",
    "        transition_incrementer -= 10\n",
    "        old_reset_x = max_reset_x\n",
    "        old_reset_y = reset_y_radius\n",
    "        max_reset_x = np.min([MAX_RESET_X,GOAL_RADIUS+max_reset_x])\n",
    "\n",
    "        if (old_reset_x != min_reset_x):\n",
    "\n",
    "            print('Making task harder')\n",
    "            print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "            parameter_decay -= 3 # Increase exploration rate a bit following the change in initialization conditions\n",
    "\n",
    "    flip = np.random.choice([True,False],1)\n",
    "    observation = environment.reset(options={'randomization_x_bounds':np.array([min_reset_x,max_reset_x]),\n",
    "                                             'randomization_y_bounds': np.array([-reset_y_radius, reset_y_radius]) + GOAL_Y,\n",
    "                                             'flip':flip})\n",
    "\n",
    "    done = False\n",
    "    while not done: # Advance the environment (e.g., the smoke plume updates and the agent walks a step)\n",
    "        explore = rng.random() < epsilon# Can pick all random numbers at start\n",
    "        if explore:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[tuple(observation)])\n",
    "\n",
    "        new_observation, reward, done, odor_measures = environment.step(action)\n",
    "        if reward > 0:\n",
    "            transition_incrementer += 1\n",
    "            parameter_decay +=1\n",
    "            total_rewards += 1\n",
    "            print('received reward')\n",
    "        update_index = tuple(np.append(observation,action))\n",
    "        t1_value_index = tuple(new_observation)# Note the use of this index requires actions to be last axes of q table\n",
    "        q_table[update_index] = \\\n",
    "            q_table[update_index] +\\\n",
    "            ALPHA * (reward + GAMMA*np.max(q_table[t1_value_index]) -\\\n",
    "            q_table[update_index])\n",
    "        observation = new_observation\n",
    "\n",
    "\n",
    "        epsilon = MIN_EPSILON + (MAX_EPSILON-MIN_EPSILON)*np.exp(-DECAY*parameter_decay)\n",
    "np.save(save_path,q_table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\n",
    "Below, save frames of a movie depicting the trained agent walking through a smoke plume according its learned policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import src.visualization.visualize_behavior\n",
    "\n",
    "POLICY_FRAMES_PARENT_DIRECTORY = 'q_policy_example_frames_YY_MM_DD'\n",
    "\n",
    "policy_movie_path = os.path.join('..','result_images',POLICY_FRAMES_PARENT_DIRECTORY)\n",
    "\n",
    "src.visualization.visualize_behavior.main(movie_path=plume_movie_path,q_table_path=save_path,savepath=policy_movie_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
