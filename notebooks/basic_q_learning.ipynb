{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimize Q lookup table and visualize policy\n",
    "## Overview\n",
    "This notebook creates an environment with a smoke plume movie and an agent that learns to seek odor sources using RL. It's useful to see how the relevant classes are created. Q values for state-action pairs are optimized in loop in the notebook, so it also demonstrates this simple learning rule.\n",
    "## Arena\n",
    "The agent navigates an arena that is 'odorized' according to a smoke plume movie ('project_root/src/data/plume_movies/intermiitent_smoke.avi').\n",
    "## State space\n",
    "The agent detects:\n",
    "    - odor concentration (binarized into low and high)\n",
    "    - odor gradient in the cross-wind direction (discretized into crosswind A, crosswind B, neither)\n",
    "    - and odor motion in the cross-wind direction (crosswind A, crosswind B, neither).\n",
    "## Action space\n",
    "The agent walks with constant speed and chooses a walking direction at every timestep. The walking directions are the upwind/downwind/crosswind A/crosswind B directions, as well as the intermediate directions (i.e., at 45 degree angles).\n",
    "## Reward\n",
    "The agent receives a reward quantity at every timepoint. This is 1 on timesteps where the agent gets to the goal zone; otherwise it is 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 3]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tqdm.notebook\n",
    "\n",
    "from src.models.goals import GOAL_X,GOAL_Y,GOAL_RADIUS\n",
    "from src.models.motion_environment_factory import PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardFactory\n",
    "\n",
    "N_EPISODES = 2000 # How many independently initialized runs to train on\n",
    "ALPHA = 0.1 # Learning rate\n",
    "GAMMA = 0.5 # Reward temporal discount factor\n",
    "MAX_EPSILON = 1 # Starting exploration rate\n",
    "MIN_EPSILON = 0.1 # Asymptote of decaying exploration rate\n",
    "DECAY = 0.1 # Rate of exploration decay\n",
    "\n",
    "MIN_RESET_X = GOAL_X + 10 + GOAL_RADIUS # Initialization condition\n",
    "MAX_RESET_X = 1430 # Initialization condition\n",
    "rewards = np.zeros(N_EPISODES)\n",
    "total_rewards = 0\n",
    "\n",
    "\n",
    "plume_movie_path = os.path.join('..', 'src', 'data', 'plume_movies', 'intermittent_smoke.avi')\n",
    "environment = PlumeMotionNavigationEnvironmentMovie1PlumeSourceRewardFactory(movie_file_path=plume_movie_path).plume_environment\n",
    "q_shape = np.append(environment.observation_space.nvec,environment.action_space.n)\n",
    "q_table = np.zeros(shape=q_shape)\n",
    "print(environment.observation_space.nvec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting less than 230 away in x coordinate\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b718455bf4fd4e139f4d83d4119f8f10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 42\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(q_table[\u001B[38;5;28mtuple\u001B[39m(observation)])\n\u001B[0;32m---> 42\u001B[0m new_observation, reward, done, odor_measures \u001B[38;5;241m=\u001B[39m \u001B[43menvironment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reward \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     44\u001B[0m     transition_incrementer \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Dropbox (Yale University)/users/sam_brudner/analysis/plume_rl/src/models/gym_motion_environment_classes.py:102\u001B[0m, in \u001B[0;36mPlumeMotionNavigationEnvironment.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;124;03mAdvance the environment, based on an action selected by the agent.\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03mReturn the next observation, the reward, a boolean indicating whether\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03mthe trial is over, and a dictionary of any additional info that might be needed\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    101\u001B[0m walk_action \u001B[38;5;241m=\u001B[39m WalkActionEnum(action)\n\u001B[0;32m--> 102\u001B[0m walk_displacement \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwalk_functions\u001B[49m[walk_action]\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_trial_walk_displacement \u001B[38;5;241m=\u001B[39m walk_displacement  \u001B[38;5;66;03m# This can be removed\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfly_spatial_parameters\u001B[38;5;241m.\u001B[39mupdate_position(walk_displacement)\n",
      "File \u001B[0;32m~/Dropbox (Yale University)/users/sam_brudner/analysis/plume_rl/src/models/gym_motion_environment_classes.py:136\u001B[0m, in \u001B[0;36mPlumeMotionNavigationEnvironment.walk_functions\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwalk_functions\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 136\u001B[0m     walk_functions \u001B[38;5;241m=\u001B[39m \u001B[43mWalkDisplacements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwind_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwind_directions\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mwalk_displacements\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m walk_functions\n",
      "File \u001B[0;32m~/Dropbox (Yale University)/users/sam_brudner/analysis/plume_rl/src/models/action_definitions.py:100\u001B[0m, in \u001B[0;36mWalkDisplacements.__init__\u001B[0;34m(self, wind_params)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworking_walk_angle \u001B[38;5;241m=\u001B[39m AngleField()\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params: WindDirections \u001B[38;5;241m=\u001B[39m wind_params\n\u001B[0;32m--> 100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwalk_displacements: \u001B[38;5;28mdict\u001B[39m[WalkActionEnum, np\u001B[38;5;241m.\u001B[39mndarray] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_walk_displacements\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dropbox (Yale University)/users/sam_brudner/analysis/plume_rl/src/models/action_definitions.py:117\u001B[0m, in \u001B[0;36mWalkDisplacements.create_walk_displacements\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_walk_displacements\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    107\u001B[0m     walk_displacements \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    108\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mUPWIND: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplacement_from_angles(wind_directions_to_combine\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mupwind]),\n\u001B[1;32m    109\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mDOWNWIND: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplacement_from_angles(\n\u001B[1;32m    110\u001B[0m             wind_directions_to_combine\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mwind_angle]),\n\u001B[1;32m    111\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mCROSS_A: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplacement_from_angles(\n\u001B[1;32m    112\u001B[0m             wind_directions_to_combine\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mcrosswind_a]),\n\u001B[1;32m    113\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mCROSS_B: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplacement_from_angles(\n\u001B[1;32m    114\u001B[0m             wind_directions_to_combine\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mcrosswind_b]),\n\u001B[1;32m    115\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mUP_A: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplacement_from_angles(\n\u001B[1;32m    116\u001B[0m             wind_directions_to_combine\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mupwind, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mcrosswind_a]),\n\u001B[0;32m--> 117\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mUP_B: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplacement_from_angles\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwind_directions_to_combine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwind_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupwind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwind_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrosswind_b\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    119\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mDOWN_A: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplacement_from_angles(\n\u001B[1;32m    120\u001B[0m             wind_directions_to_combine\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mwind_angle, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mcrosswind_a]),\n\u001B[1;32m    121\u001B[0m         WalkActionEnum\u001B[38;5;241m.\u001B[39mDOWN_B: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplacement_from_angles(\n\u001B[1;32m    122\u001B[0m             wind_directions_to_combine\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mwind_angle, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwind_params\u001B[38;5;241m.\u001B[39mcrosswind_b])\n\u001B[1;32m    123\u001B[0m     }\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m walk_displacements\n",
      "File \u001B[0;32m~/Dropbox (Yale University)/users/sam_brudner/analysis/plume_rl/src/models/action_definitions.py:103\u001B[0m, in \u001B[0;36mWalkDisplacements.displacement_from_angles\u001B[0;34m(self, wind_directions_to_combine)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplacement_from_angles\u001B[39m(\u001B[38;5;28mself\u001B[39m, wind_directions_to_combine: \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworking_walk_angle \u001B[38;5;241m=\u001B[39m \u001B[43mangle_average\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwind_directions_to_combine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwalk_speed \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39marray([np\u001B[38;5;241m.\u001B[39mcos(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworking_walk_angle), np\u001B[38;5;241m.\u001B[39msin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworking_walk_angle)])\n",
      "File \u001B[0;32m~/Dropbox (Yale University)/users/sam_brudner/analysis/plume_rl/src/models/action_definitions.py:17\u001B[0m, in \u001B[0;36mangle_average\u001B[0;34m(angle_list)\u001B[0m\n\u001B[1;32m     15\u001B[0m x_average \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean([np\u001B[38;5;241m.\u001B[39mcos(angle) \u001B[38;5;28;01mfor\u001B[39;00m angle \u001B[38;5;129;01min\u001B[39;00m angle_list])\n\u001B[1;32m     16\u001B[0m y_average \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean([np\u001B[38;5;241m.\u001B[39msin(angle) \u001B[38;5;28;01mfor\u001B[39;00m angle \u001B[38;5;129;01min\u001B[39;00m angle_list])\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m standardize_angle(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mangle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_average\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43mj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_average\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mangle\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/.conda/envs/plume_rl/lib/python3.10/site-packages/numpy/lib/function_base.py:1601\u001B[0m, in \u001B[0;36mangle\u001B[0;34m(z, deg)\u001B[0m\n\u001B[1;32m   1597\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_angle_dispatcher\u001B[39m(z, deg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1598\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (z,)\n\u001B[0;32m-> 1601\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_angle_dispatcher)\n\u001B[1;32m   1602\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mangle\u001B[39m(z, deg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1603\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1604\u001B[0m \u001B[38;5;124;03m    Return the angle of the complex argument.\u001B[39;00m\n\u001B[1;32m   1605\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1638\u001B[0m \n\u001B[1;32m   1639\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1640\u001B[0m     z \u001B[38;5;241m=\u001B[39m asanyarray(z)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "Q_SAVE_NAME = 'q_table_YY_MM_DD.npy'\n",
    "\n",
    "save_path = os.path.join('..','trained_models',Q_SAVE_NAME)\n",
    "\n",
    "epsilon = MAX_EPSILON\n",
    "alpha = ALPHA\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "min_reset_x = MIN_RESET_X\n",
    "max_reset_x = min_reset_x + GOAL_RADIUS\n",
    "print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "reset_y_radius = 400\n",
    "transition_incrementer = 0\n",
    "parameter_decay = 0\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "for episode in tqdm.notebook.tqdm(range(N_EPISODES)):\n",
    "    if (transition_incrementer > 0) & (transition_incrementer % 10 == 0):\n",
    "        transition_incrementer -= 10\n",
    "        old_reset_x = max_reset_x\n",
    "        old_reset_y = reset_y_radius\n",
    "        max_reset_x = np.min([MAX_RESET_X,GOAL_RADIUS+max_reset_x])\n",
    "\n",
    "        if (old_reset_x != min_reset_x):\n",
    "\n",
    "            print('Making task harder')\n",
    "            print(f'starting less than {max_reset_x} away in x coordinate')\n",
    "            parameter_decay -= 3 # Increase exploration rate a bit following the change in initialization conditions\n",
    "\n",
    "    flip = np.random.choice([True,False],1)\n",
    "    observation = environment.reset(options={'randomization_x_bounds':np.array([min_reset_x,min_reset_x+GOAL_RADIUS]),\n",
    "                                             'randomization_y_bounds': np.array([-reset_y_radius, reset_y_radius]) + GOAL_Y,\n",
    "                                             'flip':flip})\n",
    "\n",
    "    done = False\n",
    "    while not done: # Advance the environment (e.g., the smoke plume updates and the agent walks a step)\n",
    "        explore = rng.random() < epsilon# Can pick all random numbers at start\n",
    "        if explore:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[tuple(observation)])\n",
    "\n",
    "        new_observation, reward, done, odor_measures = environment.step(action)\n",
    "        if reward > 0:\n",
    "            transition_incrementer += 1\n",
    "            parameter_decay +=1\n",
    "            total_rewards += 1\n",
    "            print('received reward')\n",
    "        update_index = tuple(np.append(observation,action))\n",
    "        t1_value_index = tuple(new_observation)# Note the use of this index requires actions to be last axes of q table\n",
    "        q_table[update_index] = \\\n",
    "            q_table[update_index] +\\\n",
    "            ALPHA * (reward + GAMMA*np.max(q_table[t1_value_index]) -\\\n",
    "            q_table[update_index])\n",
    "        observation = new_observation\n",
    "\n",
    "\n",
    "        epsilon = MIN_EPSILON + (MAX_EPSILON-MIN_EPSILON)*np.exp(-DECAY*parameter_decay)\n",
    "np.save(save_path,q_table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\n",
    "Below, save frames of a movie depicting the trained agent walking through a smoke plume according its learned policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import src.visualization.visualize_behavior\n",
    "\n",
    "POLICY_FRAMES_PARENT_DIRECTORY = 'q_policy_example_frames_YY_MM_DD'\n",
    "\n",
    "policy_movie_path = os.path.join('..','result_images',POLICY_FRAMES_PARENT_DIRECTORY)\n",
    "\n",
    "src.visualization.visualize_behavior.main(movie_path=plume_movie_path,q_table_path=save_path,savepath=policy_movie_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
